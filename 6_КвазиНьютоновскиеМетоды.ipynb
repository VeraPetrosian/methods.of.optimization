{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание 6, Ускорение и оптимальные методы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deadline - 25.10.2024    23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовительная часть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "как и ранее, рассмотрит задачу минимизации эмпирического риска:\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{x \\in \\mathbb{R}^d} \\left[ f(x) = \\frac{1}{n} \\sum\\limits_{i=1}^n \\ell_x(a_i, b_i) + \\frac{\\lambda}{2} \\| x \\|^2_2\\right],\n",
    "\\end{equation}\n",
    "\n",
    "где:\n",
    "- $\\ell_x(a_i, b_i)$ — функция потерь (cross-entropy loss),\n",
    "- $x$ — вектор параметров модели,\n",
    "- $\\{a_i, b_i\\}_{i=1}^n$ — выборка данных,\n",
    "- $\\lambda > 0$ — параметр регуляризации.\n",
    "\n",
    "Функция потерь для каждого объекта $i$ записывается как:\n",
    "\n",
    "\\begin{equation}\n",
    "\\ell_x(a_i, b_i) = -b_i \\ln(p(x^Ta_i)) - (1 - b_i) \\ln(1 - p(x^Ta_i)),\n",
    "\\end{equation}\n",
    "\n",
    "где $p(x^Ta_i)$ — это вероятность, вычисляемая с помощью логистической функции в комбинации с линейной моделью:\n",
    "\n",
    "\\begin{equation}\n",
    "p(x^Ta_i) = \\frac{1}{1 + \\exp(-x^T a_i)}.\n",
    "\\end{equation}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К заданию приложен датасет _mushrooms_. С помощью следующего кода сформируйте матрицу $A$ и вектор $b$, в которой и будет храниться выборка $\\{a_i, b_i\\}_{i=1}^n$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"mushrooms.txt\" \n",
    "#файл должен лежать в той же деректории, что и notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "data = load_svmlight_file(dataset)\n",
    "A, b = data[0].toarray(), data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поменяем вектор $y$, чтобы $y_i$ принимали значения $0$ и $1$. Вы также можете сделать дополнительную предобработку данных (приемами из машинного обучения), но это никак дополнительно не оценивается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = b - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим данные на две части: обучающую и тестовую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "A_train, A_test, b_train, b_test = train_test_split(A, b, test_size=0.2, random_state=GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучающей части $A_{train}$, $b_{train}$ оцените константу $L$. Задайте $\\lambda$ так, чтобы $\\lambda \\approx L / 1000$.  Реализуйте в коде подсчет значения градиента для нашей целевой функции ($A$, $b$, $\\lambda$ лучше подавать в качестве параметра, чтобы была возможность их менять, а не только подставлять фиксированные $A_{train}$, $b_{train}$). Можно использовать как библиотеку ``numpy``, так и библиотеки ``autograd``, ``pytorch``, ``jax``. Воспользуйтесь кодом с предыдущего домашнего задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основная часть (всего 5 баллов) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задача 1. (всего 2.5 балла)__ Метод Ньютона (классический и демпфированный)\n",
    "\n",
    "Рассмотрим один из самых известных методов второго порядка - Метод Ньютона (Newton's method). А точнее его модификацию - демпфированный метод Ньютона (Dumped Newton's method, DNM). \n",
    "\n",
    "**Псевдокод алгоритма**\n",
    "\n",
    "_Инициализация:_\n",
    "\n",
    "Величина шага $\\{ \\gamma_k \\}_{k=0} > 0$, стартовая точка $ x^0 \\in \\mathbb{R}^d $, количество итераций $ K $\n",
    "\n",
    "$k \\hspace{-1em}$ _--ая итерация:_\n",
    "1. Подсчитать направление спуска $$ d_k = \\left( \\nabla^2 f(x^k) \\right)^{-1} \\nabla f(x_k)  $$\n",
    "2. Сделать шаг алгоритма $$ x^{k+1} = x^k - \\gamma_k d_k $$\n",
    "\n",
    "Используйте предложенную функцию для реализации алгоритма и допишите недостающие фрагменты. После чего для проверки правильности загрузите функцию в [контест](https://contest.yandex.ru/contest/66540/enter/)\n",
    "\n",
    "Как нетрудно заметить, при $\\gamma_k \\equiv 1$ демпфированный метод вырождается в классический (который, наверное, вам известен из вычислительной математики). \n",
    "\n",
    "### Указание: поставьте во всех методах точность по умолчанию равной $10^{-16}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__а) (0.25 балла)__ Для метода Ньютона нам нужно будет использовать значение гессиана функции потерь. Воспользуясь знаниями, полученными ранее, докажите, что градиент и гессиан равны, соответственно\n",
    "$$\n",
    "\\nabla f(x) = \\frac{1}{n} \\sum_{i=1}^n (p(x^Ta_i) - b_i) a_i + \\lambda x \\qquad \\qquad \\nabla^2 f(x) = \\frac{1}{n} \\sum_{i=1}^n \\big(p(x^Ta_i)(1 - p(x^Ta_i))\\big) a_i a_i^T + \\lambda I_d\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ваше решение__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__б) (0.25 балла)__ Запустите демпфированный метод Ньютона для задачи логистической регрессии. Пусть $x_0 = 0$. Постройте график зависимости критерия от числа итераций (```max_iter``` поставьте не больше 10). Сходится ли метод?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "def DumpedNewton(grad, hess, criterion, x_0, eps, max_iter, **params):\n",
    "\n",
    "    '''\n",
    "       grad(x) - функция, которая считает градиент целевой функции;\n",
    "       hess(x) - функция, которая считает гессиан целевой функции;\n",
    "       criterion(x) - функция, которая считает критерий;\n",
    "       x_0 - начальная точка;\n",
    "       eps - точность сходимости (обычно 1e-8);\n",
    "       max_iter - количество итераций;\n",
    "       **params - содержит именнованные гиперпараметры метода:\n",
    "           params['gamma'] - шаг;\n",
    "           params['flag] -- флаг продолжения вычислений.\n",
    "    '''\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    x_k = np.copy(x_0)\n",
    "    err_x_0 = criterion(x_k)\n",
    "\n",
    "    if params['flag']:\n",
    "        errors.append(criterion(x_k))\n",
    "    else:\n",
    "        errors.append(criterion(x_k) / err_x_0)\n",
    "\n",
    "    for k in trange(max_iter):\n",
    "            \n",
    "        # Ваше решение\n",
    "\n",
    "        if errors[-1] < eps:\n",
    "            break\n",
    "            \n",
    "    return x_k, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваше решение (Code + Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__в) (0.25 балла)__ Зафиксировав значение $\\gamma_k=1$, рассмотрите следующие значения парметра $x_0$:\n",
    "1. Вектор из 0;\n",
    "2. Вектор из 1;\n",
    "3. Вектора $\\vec{e}_i$ ($i = 1, 2, \\ldots , 8$), $j$-ая координата которых подчиняется следующему правилу:\n",
    "$$  \\left[ \\vec{e}_i \\right]_j = \\begin{cases} i, & \\text{если } j \\leq i \\\\ 0, &\\text{иначе} \\end{cases}$$\n",
    "\n",
    "Постройте графики зависимости критерия от числа итераций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваше решение (Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__г) (0.5 балла)__ Выберите и зафиксируйте произвольное значение $x_0$ из предыдущего пункта, для которого наблюдается сходимость DNM. Теперь исследуем зависимость сходимости от параметра $\\gamma_k$. Для этого проверьте следующие значения $\\gamma_k = \\{ 0.5, 1, 1.5, 2 \\}$. Постройте графики зависимости критерия от числа итераций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваше решение (Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__д) (0.75 балла)__ Давайте теперь вызовем несколько итераций метода градиентного спуска, перед использованием демпфированного метода Ньютона (воспользуйтесь результатами предыдущих домашек). Варьируя число итераций градиентного спуска (возьмите 10, 20, 50, 100), исследуйте сходимость метода DNM при фиксированных значениях $x_0$ (возьмите вектор из единиц) и $\\gamma_k = 1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваше решение (Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__е) (0.25 балла)__ Теперь рассмотрим датасет ```w8a.txt```. Его размеры несколько больше, чем у ```mushrooms.txt``` (сравните, насколько), поэтому вычисление гессиана будет занимать больше времени. Повторите предыдущий пункт, используя те же значения для количества предварительных шагов градиентного спуска, а число итераций демпфированного метода Ньютона положите равной 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"w8a.txt\" \n",
    "\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_svmlight_file(dataset, )\n",
    "A, b = data[0].toarray(), data[1]\n",
    "\n",
    "A_train, A_test, b_train, b_test = train_test_split(A, b, test_size=0.2, random_state=GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваше решение (Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ё) (0.25 балла)__ В данном пункте нужно будет сравнить временную сложность алгоритмов первого порядка (```GradientDescent, HeavyBall, NAG```) и предложенных в данной домашней работе реализаций метода Ньютона (```DumpedNewton``` и связку ```GradientDescent + DumpedNewton```). Для всех методов выберите $x_0 = 0$, $\\varepsilon = 10^{-16}$ и следующие значения параметров:\n",
    "1. ```GradientDescent```: max_iter = 3000, $\\gamma_k = 1/L$;\n",
    "2. ```HeavyBall```: max_iter = 3000, $\\gamma_k = 1/L$, $\\tau_k = 0.9$;\n",
    "3. ```NAG```: max_iter = 3000, $\\gamma_k = 1/L$, $\\tau_k = 0.9$;\n",
    "4. ```DumpedNewton```: max_iter = 8, $\\gamma_k = 1$;\n",
    "5. ```GradientDescent + DumpedNewton```: число шагов градиентного спуска - 100, число шагов метода Ньютона - 5, параметры методов такие же, как в пунктах 1 и 4.\n",
    "\n",
    "Сделайте вывод о применимости метода Ньютона.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваше решение (Code + Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задача 2. (всего 2.5 балла)__ Квазиньютоновские методы\n",
    "\n",
    "Вычисление обратной матрицы Гесса является крайне ресурсоемким, поэтому предлагается использовать вместо ее точного значения некоторую аппроксимацию невырожденной положительно определенной матрицей. Такие методы называются *квазиньютоновскими*. В данной части будет предложен к рассмотрению один из самых популярных методов оптимизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__а) (0.5 балла)__ Реализуйте квазиньютоновский метод L-BFGS (см. [статью](https://users.iems.northwestern.edu/~nocedal/PDFfiles/limited-memory.pdf)), основанный на аппроксимации $H_k$. \n",
    "\n",
    "**Псевдокод алгоритма**\n",
    "\n",
    "_Инициализация:_\n",
    "\n",
    "Параметры линейного поиска $c_1, c_2$, аппроксимация обратного гессиана $H_0$, стартовая точка $ x^0 \\in \\mathbb{R}^d $, количество итераций $ K $\n",
    "\n",
    "$k \\hspace{-1em}$ _--ая итерация:_\n",
    "\n",
    "1. Подсчитать направление спуска, используя аппроксимацию обратного гессиана $H_k$:\n",
    "$$d_k = -H_k \\nabla f(x_k).$$\n",
    "2. Выполнить процедуру линейного поиска параметра $\\alpha_k$, удовлетворяющего условиям Вольфе (см. [определение](https://en.wikipedia.org/wiki/Wolfe_conditions)):\n",
    "$$\\begin{align*}\n",
    "f(x_k + \\alpha_k d_k) &\\leq f(x_k) + c_1 \\alpha_k d_k^T \\nabla f(x_k)\\\\\n",
    "-d_k^T \\nabla f(x_k + \\alpha_k d_k)  &\\leq -c_2 d_k^T \\nabla f(x_k)\n",
    "\\end{align*}$$ \n",
    "3. Обновить $x_k$:\n",
    "$$x_{k + 1} = x_k + \\alpha_k d_k$$\n",
    "4. Ввести $$\\begin{align*} s_k &= x_{k + 1} - x_k \\\\ y_k &= \\nabla f(x_{k + 1}) - \\nabla f(x_k)\\end{align*}$$ и обновить значение $H_k$:\n",
    "$$\n",
    "H_{k +1} = \\left( I - \\frac{y_k s_k^T}{y_k^T s_k} \\right)^T H_k \\left( I - \\frac{y_k s_k^T}{y_k^T s_k} \\right) + \\frac{y_k s_k^T}{y_k^T s_k}\n",
    "$$\n",
    "\n",
    "*Замечание*: подумайте, как лучше инициализировать матрицу $H_0$. Функцию линейного поиска можно реализовать самостоятельно, а можете воспользоваться реализацией из библиотеки ```scipy.optimize```. \n",
    "\n",
    "Используйте предложенную функцию для реализации алгоритма и допишите недостающие фрагменты. После чего для проверки правильности загрузите функцию в [контест](https://contest.yandex.ru/contest/66540/enter/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from scipy.optimize import line_search\n",
    "\n",
    "def BFGS(f, grad, H_0, x_0, criterion, eps, max_iter, **params):\n",
    "    '''\n",
    "        f(x) - оптимизируемая функция\n",
    "        grad(x) - функция, которая считает градиент целевой функции;\n",
    "        H_0 - аппроксимация гессиана невырожденной положительно определенной матрицей;\n",
    "        criterion(x) - функция, которая считает критерий;\n",
    "        x_0 - начальная точка;\n",
    "        eps - точность сходимости (обычно 1e-8);\n",
    "        max_iter - количество итераций;\n",
    "        **params - содержит именнованные гиперпараметры метода:\n",
    "            params['c_1'] - параметр в условие Вольфе, по умолчанию равен 0.0001\n",
    "            params['c_2'] - параметр в условие Вольфе, по умолчанию равен 0.9\n",
    "    '''\n",
    "    errors = []\n",
    "\n",
    "    err_x_0 = criterion(x_0)\n",
    "    errors.append(criterion(x_0) / err_x_0)\n",
    "\n",
    "    H_k = np.copy(H_0)\n",
    "\n",
    "    x_k = np.array(x_0)\n",
    "    x_new = np.array(x_0)\n",
    "\n",
    "    g_k = grad(x_0)\n",
    "    g_new = grad(x_0)\n",
    "\n",
    "    I = np.eye(d)\n",
    "\n",
    "    for k in trange(max_iter):\n",
    "\n",
    "        # Ваше решение\n",
    "        \n",
    "        errors.append(criterion(x_new) / err_x_0)\n",
    "        if errors[-1] < eps:\n",
    "            break\n",
    "\n",
    "    return x_new, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вернемся к датасету ```mushrooms.txt```, для ускорения работы алгоритмов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"mushrooms.txt\" \n",
    "\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_svmlight_file(dataset)\n",
    "A, b = data[0].toarray(), data[1]\n",
    "\n",
    "b = 2 * b - 3\n",
    "\n",
    "A_train, A_test, b_train, b_test = train_test_split(A, b, test_size=0.2, random_state=GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__б) (0.5 балла)__ Рассмотрите различные значения параметра $x_0$, как в пункте __1б)__. Число итераций поставьте равным 150. Постройте график зависимости критерия от номера итерации. Проведите сравнение с подобными графиками для DNM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваше решение (Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__в) (0.75 балла)__ Однако, такое приближение гессианов не единственно. Реализуйте метод SR1 (Symmetric rank-1) (см. [статью](https://arxiv.org/pdf/2002.00657.pdf)) со следующим обновлением аппроксимации обратного гессиана:\n",
    "$$\n",
    "H_{k + 1} = H_k + \\frac{(s_k - H_k y_k)(s_k - H_k y_k)^T}{(s_k - H_k y_k)y_k^T}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from scipy.optimize import line_search\n",
    "\n",
    "def SR1(f, grad, H_0, x_0, criterion, eps, max_iter, **params):\n",
    "    '''\n",
    "        f(x) - оптимизируемая функция\n",
    "        grad(x) - функция, которая считает градиент целевой функции;\n",
    "        H_0 - аппроксимация гессиана невырожденной положительно определенной матрицей;\n",
    "        criterion(x) - функция, которая считает критерий;\n",
    "        x_0 - начальная точка;\n",
    "        eps - точность сходимости (обычно 1e-8);\n",
    "        max_iter - количество итераций;\n",
    "        **params - содержит именнованные гиперпараметры метода:\n",
    "            params['c_1'] - параметр в условие Вольфе, по умолчанию равен 0.0001\n",
    "            params['c_2'] - параметр в условие Вольфе, по умолчанию равен 0.9\n",
    "    '''\n",
    "    errors = []\n",
    "\n",
    "    err_x_0 = criterion(x_0)\n",
    "    errors.append(criterion(x_0) / err_x_0)\n",
    "\n",
    "    H_k = np.copy(H_0)\n",
    "\n",
    "    x_k = np.array(x_0)\n",
    "    x_new = np.array(x_0)\n",
    "\n",
    "    g_k = grad(x_0)\n",
    "    g_new = grad(x_0)\n",
    "\n",
    "    for k in trange(max_iter):\n",
    "\n",
    "        # Ваше решение\n",
    "\n",
    "        errors.append(criterion(x_new) / err_x_0)\n",
    "        if errors[-1] < eps:\n",
    "            break\n",
    "\n",
    "    return x_new, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__г) (0.75 балла)__ Теперь давайте сравним работу квазиньютоновских методов с ускоренным методом (возьмите NAG) и методом второго порядка (DumpedNewton). В качестве начальной точки выберите $x_0 = 0$. Постройте сравнительные графики зависимостей критерия от итерации и критерия от времени. Что можно сказать о применимости квазиньютоновских методов? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваше решение (Code + Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительная часть (всего 5 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задача 3. (всего 5 баллов)__ Квазиньютоновские методы очень хороши, так как не требуют использование второго порядка гладкости. Однако, все равно приходится хранить приближения матрицы Гесса, что при больших размерностях задачи очень сильно влияет на асимптотику по памяти, делая ее квадратичной. Для оптимизации этой величины были представлены методы с представкой L- (L-BFGS, L-SR1), где L- означает limited memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a) (1 балл)__ Реализуйте метод L-BFGS, использующий технику ограниченной памяти с асимптотикой $\\mathcal{O}(n)$. Пусть разрешено использовать $m$ значений с предыдущих итераций. Тогда псевдокод на $k$-ой итерации:\n",
    "\n",
    "**Псевдокод алгоритма**\n",
    "\n",
    "_Инициализация:_\n",
    "\n",
    "Параметры линейного поиска $c_1, c_2$, стартовая аппроксимация обратного гессиана $H_0^k$, стартовая точка $ x^0 \\in \\mathbb{R}^d $, количество итераций $ K $\n",
    "\n",
    "$k \\hspace{-1em}$ _--ая итерация:_\n",
    "\n",
    "1. Создать переменную $q = g_k$.\n",
    "2. Для $i = k - 1, \\ldots, k - m$ уточнить аппроксимацию градиента $q$, сохранив при этом вычисленное значение $\\alpha_i$:\n",
    "$$\n",
    "\\alpha_i =\\rho_i s_i^T q \\qquad \\qquad q = q - \\alpha_i y_i.\n",
    "$$\n",
    "3. Задать нулевое приблежение обратного гессиана $H_k^0$ (как в статье, вы же можете рассмотреть другое):\n",
    "$$\n",
    "H_k^0 = \\frac{s_{k-1}^T y_{k-1}}{y_{k-1}^T y_{k-1}} I.\n",
    "$$\n",
    "4. Вычислить значение спуска:\n",
    "$$\n",
    "r = H_k^0 q\n",
    "$$\n",
    "5. Для $i = k - m, \\ldots, k - 1$ уточнить значение вектора спуска $r$:\n",
    "$$\n",
    "r = r + s_i (\\alpha_i - \\rho_i y_i^T r).\n",
    "$$\n",
    "6. Сделать стандартные обновления для подсчета $x_{k + 1}, s_k, y_k, \\rho_k$, сохранить их в память. Если память переполнена -- убрать элемент с индексом 0 (то есть тот, который был положен раньше остальных).\n",
    "\n",
    "Используйте предложенную функцию для реализации алгоритма и допишите недостающие фрагменты. После чего для проверки правильности загрузите функцию в [контест](https://contest.yandex.ru/contest/66540/enter/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def LBFGS(f, grad, H_0, x_0, mem_size, criterion, eps, max_iter, **params):\n",
    "    '''\n",
    "        f(x) - оптимизируемая функция\n",
    "        grad(x) - функция, которая считает градиент целевой функции;\n",
    "        H_0 - аппроксимация гессиана невырожденной положительно определенной матрицей;\n",
    "        mem_size - количество выделенной памяти, по умолчанию 1;\n",
    "        criterion(x) - функция, которая считает критерий;\n",
    "        x_0 - начальная точка;\n",
    "        eps - точность сходимости (обычно 1e-8);\n",
    "        max_iter - количество итераций;\n",
    "        **params - содержит именнованные гиперпараметры метода:\n",
    "            params['c_1'] - параметр в условие Вольфе, по умолчанию равен 0.0001\n",
    "            params['c_2'] - параметр в условие Вольфе, по умолчанию равен 0.9\n",
    "    '''\n",
    "    errors = []\n",
    "    time_logs = []\n",
    "\n",
    "    err_x_0 = criterion(x_0)\n",
    "    errors.append(criterion(x_0) / err_x_0)\n",
    "\n",
    "    H_k = np.copy(H_0)\n",
    "\n",
    "    x_k = np.array(x_0)\n",
    "    x_new = np.array(x_0)\n",
    "\n",
    "    g_k = grad(x_0)\n",
    "    g_new = grad(x_0)\n",
    "\n",
    "    I = np.eye(d)\n",
    "\n",
    "    time_logs.append(0)\n",
    "\n",
    "    x_diffs = []\n",
    "    grad_diffs = []\n",
    "    rhos = []\n",
    "\n",
    "    for k in trange(max_iter):\n",
    "\n",
    "        # Ваше решение\n",
    "\n",
    "        if errors[-1] < eps:\n",
    "            break\n",
    "\n",
    "    return x_new, errors, time_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__б) (0.5 балла)__ Сравните сходимость метода при разных значениях параметра ```mem_size```. Рассмотрите следующие значения: 1, 10, 100, 1000, 10000. Постройте графики зависимости критерия от номера итерации и критерия от времени. Сравните также с BFGS. Что можно сказать об алгоритме с ограниченной памятью?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваше решение (Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__в)__ __(2 балла)__ До этого мы рассматривали методы, требующие липшицевость градиента. Однако, существует алгоритм, требующий липшицевость гессиана. Это метод Ньютона с кубической регуляризацией, предложенный Ю.$~$Нестеровым и Б.$~$Т.$~$Поляком в [этой работе](https://link.springer.com/article/10.1007/s10107-006-0706-8). На его $k$-ой итерации необходимо отрешивать следующую подзадачу:\n",
    "$$\n",
    "x_{k + 1} = \\arg \\min \\limits_{x \\in \\mathbb{R}^d} \\left\\{ \\langle \\nabla f(x_k), x - x_k \\rangle + \\frac{1}{2} \\langle \\nabla^2 f(x_k) (x - x_k), x - x_k \\rangle + \\frac{M_k}{6} \\| x - x_k\\|_2^2 \\right\\}\n",
    "$$\n",
    "Здесь $M_k$ -- константа Липшица гессиана. Так как в общем случае вычислять тензор третьего ранга не очень выгодно, предлагается рассмотреть адаптивную константу, которую мы будем задавать как\n",
    "$$\n",
    "M_k = \\frac{\\alpha}{\\left( 1 + \\beta^{1 + k} \\right)},\n",
    "$$\n",
    "константы $\\alpha$ и $\\beta$ для начала можно положить равными 0.005 и 0.15, соответственно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_cubic(x_k, f, g_k, H_k, M, criterion, eps=1e-8, max_iter=10):\n",
    "    '''\n",
    "        x_k - точка, в которой решается задача минимизации;\n",
    "        f(x) - целевая функция:\n",
    "        g_k - значение градиента функции в точке x_k;\n",
    "        H_k - значение гессиана функции в точке x_k;\n",
    "        M - константа Липшица гессиана;\n",
    "        criterion(x, y) - функция, считающая критерий останова, в оригинальной статье взят 1/||x|| - 1/||y||;\n",
    "        eps - точность сходимости (обычно 1e-8);\n",
    "        max_iter - количество итераций для решения подзадачи, по умолчанию 10;\n",
    "    '''\n",
    "    # Inner variable to trace amount of steps made inside the subproblem\n",
    "    solver_it = 1\n",
    "\n",
    "    newton_step = -np.linalg.solve(H_k, g_k)\n",
    "    if M == 0:\n",
    "        return x_k + newton_step, solver_it\n",
    "\n",
    "    def cauchy_point(g, H, M):\n",
    "        if np.linalg.norm(g) == 0 or M == 0:\n",
    "            return 0 * g\n",
    "\n",
    "        g_dir = g / np.linalg.norm(g)\n",
    "        H_g_g = H @ g_dir @ g_dir\n",
    "        R = -H_g_g / (2*M) + np.sqrt((H_g_g/M)**2 / 4 + np.linalg.norm(g)/M)\n",
    "        return -R * g_dir\n",
    "\n",
    "    r_min = np.linalg.norm(cauchy_point(g_k, H_k, M))\n",
    "\n",
    "    x_new = x_k + newton_step\n",
    "    if f(x_k) > f(x_new):\n",
    "        return x_new, solver_it\n",
    "\n",
    "    r_max = np.linalg.norm(newton_step)\n",
    "    if r_max - r_min < eps:\n",
    "        return x_k + newton_step, solver_it\n",
    "\n",
    "    I = np.eye(len(g_k))\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        r_try = (r_min + r_max) / 2\n",
    "        lam = r_try * M\n",
    "        s_lam = -np.linalg.solve(H_k + lam * I, g_k)\n",
    "        solver_it += 1\n",
    "\n",
    "        crit = 1./np.linalg.norm(s_lam) - 1./np.linalg.norm(r_try)\n",
    "        if np.abs(crit) < eps:\n",
    "            return x_k + s_lam, solver_it\n",
    "\n",
    "        if crit < 0:\n",
    "            r_min = r_try\n",
    "        else:\n",
    "            r_max = r_try\n",
    "        if r_max - r_min < eps:\n",
    "            break\n",
    "    return x_k + s_lam, solver_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def CubicNewton(f, grad, H, x_0, criterion, eps, max_iter, **params):\n",
    "    '''\n",
    "        f(x) - оптимизируемая функция\n",
    "        grad(x) - функция, которая считает градиент целевой функции;\n",
    "        H(x) - функция, которая считает гессиан целевой функции;\n",
    "        criterion(x) - функция, которая считает критерий;\n",
    "        x_0 - начальная точка;\n",
    "        eps - точность сходимости (обычно 1e-8);\n",
    "        max_iter - количество итераций;\n",
    "        **params - содержит именнованные гиперпараметры метода:\n",
    "            params['M'] - константа Липшица гессиана, она же константа кубического регуляризатора\n",
    "    '''\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    err_x_0 = criterion(x_0)\n",
    "    errors.append(criterion(x_0) / err_x_0)\n",
    "\n",
    "    x_k = np.array(x_0)\n",
    "    g_k = grad(x_0)\n",
    "    H_k = H(x_0)\n",
    "\n",
    "    for k in tqdm(range(max_iter)):\n",
    "\n",
    "        M_k = params['M'](k)\n",
    "\n",
    "        x_k, solver_it = solve_cubic(x_k, f, g_k, H_k, M_k, criterion, eps=1e-16, max_iter=2)\n",
    "        k += solver_it\n",
    "        g_k = grad(x_k)\n",
    "        H_k = H(x_k)\n",
    "\n",
    "        errors.append(criterion(x_k) / err_x_0)\n",
    "        if errors[-1] < eps:\n",
    "            break\n",
    "\n",
    "    return x_k, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__г) (0.25 балла)__ Примените кубически регуляризованный метод Ньютона для ERM. Постройте график зависимости значения критерия от итерации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Решение (Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__д) (0.25 балла)__ Давайте подберем теперь наилучшие значения параметров $\\alpha, \\beta$, обеспечивающих наискорейшую сходимость метода. Для этого рассмотрите $\\alpha = \\{ 10^{-1}, 10^{-2}, 10^{-3}, 10^{-4} \\}$ и $\\beta = \\{ 1, 0.5, 0.1, 0.05 \\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваше решение (Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__е) (1 балл)__ И финальное сравнение - постройте сравнительные графики для всех изученных методов в данной домашней работе. Параметры подберите так, чтобы они обеспечивали наилучшую сходимость методов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваше решение (Code + Markdown)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
